{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from time import time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T05:26:28.988676Z",
     "start_time": "2024-01-31T05:26:26.401582700Z"
    }
   },
   "id": "91e1f74f625086a9",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files created.\n"
     ]
    }
   ],
   "source": [
    "def create_csv(root_folder, output_csv):\n",
    "    with open(output_csv, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['image_filename', 'label'])\n",
    "\n",
    "        for label in os.listdir(root_folder):\n",
    "            label_path = os.path.join(root_folder, label)\n",
    "            if os.path.isdir(label_path):\n",
    "                for image_filename in os.listdir(label_path):\n",
    "                    image_path = os.path.join(label, image_filename)\n",
    "                    label_value = int(label)\n",
    "                    writer.writerow([image_path, label_value])\n",
    "\n",
    "# Specify the path to your \"Train\" and \"Test\" dataset folders\n",
    "train_dataset_folder = 'Train/'\n",
    "test_dataset_folder = 'Test/'\n",
    "\n",
    "# Create the CSV files\n",
    "create_csv(train_dataset_folder, \"Train/train.csv\")\n",
    "create_csv(test_dataset_folder, \"Test/test.csv\")\n",
    "\n",
    "print('CSV files created.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T05:26:29.645456800Z",
     "start_time": "2024-01-31T05:26:29.586668500Z"
    }
   },
   "id": "e17880ebb31181ab",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class AIDER(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "\n",
    "        # Use PIL to open and transform the image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        label = int(self.img_labels.iloc[idx, 1])  # Convert label to integer\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        # Convert label to torch.LongTensor\n",
    "        label = torch.LongTensor([label])\n",
    "        return image, label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T05:26:30.346908900Z",
     "start_time": "2024-01-31T05:26:30.339160800Z"
    }
   },
   "id": "e2348f69b1528ad4",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\최신우/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "C:\\Users\\최신우\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\최신우\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\최신우\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\googlenet.py:47: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'googlenet', pretrained=False)\n",
    "in_features = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(in_features, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T05:26:31.726813800Z",
     "start_time": "2024-01-31T05:26:31.578346400Z"
    }
   },
   "id": "916b9f2cecb1b27d",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "training_data = AIDER(\n",
    "    annotations_file=\"Train/train.csv\",\n",
    "    img_dir=\"Train/\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    ")\n",
    "\n",
    "test_data = AIDER(\n",
    "    annotations_file=\"Test/test.csv\",\n",
    "    img_dir=\"Test/\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=32, shuffle=True, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T05:26:36.481122Z",
     "start_time": "2024-01-31T05:26:36.462182100Z"
    }
   },
   "id": "e81e6fbed7fa2376",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available. Switching to CPU.\n",
      "Epoch 1/20, Batch 7/161, Loss: 1.2401175924709864, Time taken since last batch: 16.16s, Total time taken: 16.16s\n",
      "Epoch 1/20, Batch 14/161, Loss: 1.0924789905548096, Time taken since last batch: 14.45s, Total time taken: 30.61s\n",
      "Epoch 1/20, Batch 21/161, Loss: 1.044010789621444, Time taken since last batch: 14.34s, Total time taken: 44.95s\n",
      "Epoch 1/20, Batch 28/161, Loss: 1.0078392390693938, Time taken since last batch: 14.59s, Total time taken: 59.54s\n",
      "Epoch 1/20, Batch 35/161, Loss: 0.9767641970089503, Time taken since last batch: 14.49s, Total time taken: 74.03s\n",
      "Epoch 1/20, Batch 42/161, Loss: 0.9345507806255704, Time taken since last batch: 14.40s, Total time taken: 88.43s\n",
      "Epoch 1/20, Batch 49/161, Loss: 0.946445035691164, Time taken since last batch: 14.57s, Total time taken: 103.00s\n",
      "Epoch 1/20, Batch 56/161, Loss: 0.9332880175539425, Time taken since last batch: 14.55s, Total time taken: 117.55s\n",
      "Epoch 1/20, Batch 63/161, Loss: 0.9247880795645336, Time taken since last batch: 14.64s, Total time taken: 132.19s\n",
      "Epoch 1/20, Batch 70/161, Loss: 0.9175253212451935, Time taken since last batch: 14.47s, Total time taken: 146.66s\n",
      "Epoch 1/20, Batch 77/161, Loss: 0.9195633762842649, Time taken since last batch: 14.15s, Total time taken: 160.81s\n",
      "Epoch 1/20, Batch 84/161, Loss: 0.9072734940619696, Time taken since last batch: 14.19s, Total time taken: 175.01s\n",
      "Epoch 1/20, Batch 91/161, Loss: 0.9057189245800396, Time taken since last batch: 14.58s, Total time taken: 189.59s\n",
      "Epoch 1/20, Batch 98/161, Loss: 0.8976320618269394, Time taken since last batch: 14.36s, Total time taken: 203.95s\n",
      "Epoch 1/20, Batch 105/161, Loss: 0.8893866879599435, Time taken since last batch: 14.74s, Total time taken: 218.68s\n",
      "Epoch 1/20, Batch 112/161, Loss: 0.8784170376935175, Time taken since last batch: 14.28s, Total time taken: 232.96s\n",
      "Epoch 1/20, Batch 119/161, Loss: 0.8697734672983154, Time taken since last batch: 14.47s, Total time taken: 247.43s\n",
      "Epoch 1/20, Batch 126/161, Loss: 0.8612781751250463, Time taken since last batch: 14.42s, Total time taken: 261.84s\n",
      "Epoch 1/20, Batch 133/161, Loss: 0.8664053505972812, Time taken since last batch: 15.01s, Total time taken: 276.86s\n",
      "Epoch 1/20, Batch 140/161, Loss: 0.865142955311707, Time taken since last batch: 14.66s, Total time taken: 291.52s\n",
      "Epoch 1/20, Batch 147/161, Loss: 0.8645331918382321, Time taken since last batch: 14.08s, Total time taken: 305.60s\n",
      "Epoch 1/20, Batch 154/161, Loss: 0.8655836022906489, Time taken since last batch: 14.42s, Total time taken: 320.02s\n",
      "Epoch 1/20, Batch 161/161, Loss: 0.8617190842672905, Time taken since last batch: 14.04s, Total time taken: 334.06s\n",
      "Epoch 2/20, Batch 7/161, Loss: 0.7586327535765511, Time taken since last batch: 13.93s, Total time taken: 13.93s\n",
      "Epoch 2/20, Batch 14/161, Loss: 0.7728963324001857, Time taken since last batch: 14.01s, Total time taken: 27.94s\n",
      "Epoch 2/20, Batch 21/161, Loss: 0.7795550312314715, Time taken since last batch: 14.13s, Total time taken: 42.07s\n",
      "Epoch 2/20, Batch 28/161, Loss: 0.8125455932957786, Time taken since last batch: 14.02s, Total time taken: 56.09s\n",
      "Epoch 2/20, Batch 35/161, Loss: 0.7935152530670166, Time taken since last batch: 14.15s, Total time taken: 70.24s\n",
      "Epoch 2/20, Batch 42/161, Loss: 0.8095739341917492, Time taken since last batch: 14.29s, Total time taken: 84.53s\n",
      "Epoch 2/20, Batch 49/161, Loss: 0.8021501120255918, Time taken since last batch: 14.22s, Total time taken: 98.75s\n",
      "Epoch 2/20, Batch 56/161, Loss: 0.7993805844868932, Time taken since last batch: 14.26s, Total time taken: 113.00s\n",
      "Epoch 2/20, Batch 63/161, Loss: 0.8031135844805884, Time taken since last batch: 13.07s, Total time taken: 126.08s\n",
      "Epoch 2/20, Batch 70/161, Loss: 0.8002738101141793, Time taken since last batch: 11.46s, Total time taken: 137.53s\n",
      "Epoch 2/20, Batch 77/161, Loss: 0.7939638712189414, Time taken since last batch: 11.77s, Total time taken: 149.30s\n",
      "Epoch 2/20, Batch 84/161, Loss: 0.7873647936752864, Time taken since last batch: 11.31s, Total time taken: 160.61s\n",
      "Epoch 2/20, Batch 91/161, Loss: 0.7862325648030082, Time taken since last batch: 11.47s, Total time taken: 172.08s\n",
      "Epoch 2/20, Batch 98/161, Loss: 0.7842091282411497, Time taken since last batch: 11.44s, Total time taken: 183.52s\n",
      "Epoch 2/20, Batch 105/161, Loss: 0.783656899985813, Time taken since last batch: 11.29s, Total time taken: 194.81s\n",
      "Epoch 2/20, Batch 112/161, Loss: 0.7952543978712389, Time taken since last batch: 11.29s, Total time taken: 206.10s\n",
      "Epoch 2/20, Batch 119/161, Loss: 0.8000854867346147, Time taken since last batch: 11.30s, Total time taken: 217.40s\n",
      "Epoch 2/20, Batch 126/161, Loss: 0.7988008206325864, Time taken since last batch: 11.18s, Total time taken: 228.59s\n",
      "Epoch 2/20, Batch 133/161, Loss: 0.7914157104223294, Time taken since last batch: 11.40s, Total time taken: 239.99s\n",
      "Epoch 2/20, Batch 140/161, Loss: 0.7936750699366842, Time taken since last batch: 11.31s, Total time taken: 251.30s\n",
      "Epoch 2/20, Batch 147/161, Loss: 0.7882703325780881, Time taken since last batch: 11.40s, Total time taken: 262.70s\n",
      "Epoch 2/20, Batch 154/161, Loss: 0.7798516071074969, Time taken since last batch: 11.39s, Total time taken: 274.09s\n",
      "Epoch 2/20, Batch 161/161, Loss: 0.7732820153606604, Time taken since last batch: 10.98s, Total time taken: 285.07s\n",
      "Epoch 3/20, Batch 7/161, Loss: 0.6693207280976432, Time taken since last batch: 11.48s, Total time taken: 11.48s\n",
      "Epoch 3/20, Batch 14/161, Loss: 0.6665875038930348, Time taken since last batch: 11.25s, Total time taken: 22.73s\n",
      "Epoch 3/20, Batch 21/161, Loss: 0.6711702403568086, Time taken since last batch: 11.36s, Total time taken: 34.08s\n",
      "Epoch 3/20, Batch 28/161, Loss: 0.7117145870413099, Time taken since last batch: 12.64s, Total time taken: 46.73s\n",
      "Epoch 3/20, Batch 35/161, Loss: 0.7375714949199131, Time taken since last batch: 11.87s, Total time taken: 58.59s\n",
      "Epoch 3/20, Batch 42/161, Loss: 0.7379602420897711, Time taken since last batch: 12.13s, Total time taken: 70.72s\n",
      "Epoch 3/20, Batch 49/161, Loss: 0.7274323373424764, Time taken since last batch: 11.67s, Total time taken: 82.39s\n",
      "Epoch 3/20, Batch 56/161, Loss: 0.7235409085239682, Time taken since last batch: 11.52s, Total time taken: 93.91s\n",
      "Epoch 3/20, Batch 63/161, Loss: 0.7036829013673086, Time taken since last batch: 12.28s, Total time taken: 106.19s\n",
      "Epoch 3/20, Batch 70/161, Loss: 0.7067209942000253, Time taken since last batch: 11.56s, Total time taken: 117.75s\n",
      "Epoch 3/20, Batch 77/161, Loss: 0.7069130508930652, Time taken since last batch: 11.57s, Total time taken: 129.32s\n",
      "Epoch 3/20, Batch 84/161, Loss: 0.7121845341864086, Time taken since last batch: 11.57s, Total time taken: 140.89s\n",
      "Epoch 3/20, Batch 91/161, Loss: 0.7084036544784085, Time taken since last batch: 11.50s, Total time taken: 152.39s\n",
      "Epoch 3/20, Batch 98/161, Loss: 0.704570939650341, Time taken since last batch: 11.60s, Total time taken: 163.99s\n",
      "Epoch 3/20, Batch 105/161, Loss: 0.7089945847079867, Time taken since last batch: 11.62s, Total time taken: 175.61s\n",
      "Epoch 3/20, Batch 112/161, Loss: 0.7040650530585221, Time taken since last batch: 11.33s, Total time taken: 186.94s\n",
      "Epoch 3/20, Batch 119/161, Loss: 0.7036612960470825, Time taken since last batch: 12.84s, Total time taken: 199.77s\n",
      "Epoch 3/20, Batch 126/161, Loss: 0.698285019113904, Time taken since last batch: 11.37s, Total time taken: 211.14s\n",
      "Epoch 3/20, Batch 133/161, Loss: 0.7034992553237686, Time taken since last batch: 11.61s, Total time taken: 222.76s\n",
      "Epoch 3/20, Batch 140/161, Loss: 0.7122165965182441, Time taken since last batch: 11.61s, Total time taken: 234.37s\n",
      "Epoch 3/20, Batch 147/161, Loss: 0.7108314094089326, Time taken since last batch: 11.94s, Total time taken: 246.31s\n",
      "Epoch 3/20, Batch 154/161, Loss: 0.7167736651835503, Time taken since last batch: 12.22s, Total time taken: 258.54s\n",
      "Epoch 3/20, Batch 161/161, Loss: 0.7174763224139717, Time taken since last batch: 11.34s, Total time taken: 269.88s\n",
      "Epoch 4/20, Batch 7/161, Loss: 0.666213835988726, Time taken since last batch: 11.65s, Total time taken: 11.65s\n",
      "Epoch 4/20, Batch 14/161, Loss: 0.655784336583955, Time taken since last batch: 11.75s, Total time taken: 23.41s\n",
      "Epoch 4/20, Batch 21/161, Loss: 0.7024153797399431, Time taken since last batch: 11.82s, Total time taken: 35.23s\n",
      "Epoch 4/20, Batch 28/161, Loss: 0.6856021380850247, Time taken since last batch: 11.59s, Total time taken: 46.82s\n",
      "Epoch 4/20, Batch 35/161, Loss: 0.6957375611577715, Time taken since last batch: 11.63s, Total time taken: 58.45s\n",
      "Epoch 4/20, Batch 42/161, Loss: 0.6927019449926558, Time taken since last batch: 11.56s, Total time taken: 70.01s\n",
      "Epoch 4/20, Batch 49/161, Loss: 0.6908289467801854, Time taken since last batch: 11.47s, Total time taken: 81.49s\n",
      "Epoch 4/20, Batch 56/161, Loss: 0.6901991420558521, Time taken since last batch: 11.70s, Total time taken: 93.18s\n",
      "Epoch 4/20, Batch 63/161, Loss: 0.6871000859472487, Time taken since last batch: 11.58s, Total time taken: 104.76s\n",
      "Epoch 4/20, Batch 70/161, Loss: 0.6684216690914971, Time taken since last batch: 11.70s, Total time taken: 116.46s\n",
      "Epoch 4/20, Batch 77/161, Loss: 0.6631137105551633, Time taken since last batch: 11.65s, Total time taken: 128.11s\n",
      "Epoch 4/20, Batch 84/161, Loss: 0.6569821816824731, Time taken since last batch: 11.55s, Total time taken: 139.66s\n",
      "Epoch 4/20, Batch 91/161, Loss: 0.6610569321847224, Time taken since last batch: 11.69s, Total time taken: 151.35s\n",
      "Epoch 4/20, Batch 98/161, Loss: 0.6601039228998885, Time taken since last batch: 11.49s, Total time taken: 162.84s\n",
      "Epoch 4/20, Batch 105/161, Loss: 0.6576429914860499, Time taken since last batch: 11.47s, Total time taken: 174.31s\n",
      "Epoch 4/20, Batch 112/161, Loss: 0.6640569352145705, Time taken since last batch: 11.70s, Total time taken: 186.02s\n",
      "Epoch 4/20, Batch 119/161, Loss: 0.6611366660154167, Time taken since last batch: 11.46s, Total time taken: 197.47s\n",
      "Epoch 4/20, Batch 126/161, Loss: 0.6539124158166704, Time taken since last batch: 11.44s, Total time taken: 208.91s\n",
      "Epoch 4/20, Batch 133/161, Loss: 0.6515718638000632, Time taken since last batch: 11.81s, Total time taken: 220.72s\n",
      "Epoch 4/20, Batch 140/161, Loss: 0.6452532480869975, Time taken since last batch: 11.77s, Total time taken: 232.49s\n",
      "Epoch 4/20, Batch 147/161, Loss: 0.64130612822617, Time taken since last batch: 11.53s, Total time taken: 244.02s\n",
      "Epoch 4/20, Batch 154/161, Loss: 0.6409130787307565, Time taken since last batch: 11.50s, Total time taken: 255.52s\n",
      "Epoch 4/20, Batch 161/161, Loss: 0.645570732236649, Time taken since last batch: 11.68s, Total time taken: 267.20s\n",
      "Epoch 5/20, Batch 7/161, Loss: 0.6941615853990827, Time taken since last batch: 11.59s, Total time taken: 11.59s\n",
      "Epoch 5/20, Batch 14/161, Loss: 0.6392481241907392, Time taken since last batch: 11.66s, Total time taken: 23.25s\n",
      "Epoch 5/20, Batch 21/161, Loss: 0.6550052847181048, Time taken since last batch: 11.59s, Total time taken: 34.83s\n",
      "Epoch 5/20, Batch 28/161, Loss: 0.6608501268284661, Time taken since last batch: 11.45s, Total time taken: 46.28s\n",
      "Epoch 5/20, Batch 35/161, Loss: 0.645461026259831, Time taken since last batch: 11.59s, Total time taken: 57.88s\n",
      "Epoch 5/20, Batch 42/161, Loss: 0.6375491214650018, Time taken since last batch: 11.52s, Total time taken: 69.39s\n",
      "Epoch 5/20, Batch 49/161, Loss: 0.6310299428141847, Time taken since last batch: 11.68s, Total time taken: 81.07s\n",
      "Epoch 5/20, Batch 56/161, Loss: 0.6341399337564196, Time taken since last batch: 11.86s, Total time taken: 92.93s\n",
      "Epoch 5/20, Batch 63/161, Loss: 0.6368107270626795, Time taken since last batch: 11.53s, Total time taken: 104.46s\n",
      "Epoch 5/20, Batch 70/161, Loss: 0.6343588786465781, Time taken since last batch: 11.73s, Total time taken: 116.19s\n",
      "Epoch 5/20, Batch 77/161, Loss: 0.6405515585626874, Time taken since last batch: 11.49s, Total time taken: 127.68s\n",
      "Epoch 5/20, Batch 84/161, Loss: 0.632584083647955, Time taken since last batch: 11.74s, Total time taken: 139.42s\n",
      "Epoch 5/20, Batch 91/161, Loss: 0.6305338809123406, Time taken since last batch: 11.57s, Total time taken: 150.99s\n",
      "Epoch 5/20, Batch 98/161, Loss: 0.6258227043614095, Time taken since last batch: 11.60s, Total time taken: 162.59s\n",
      "Epoch 5/20, Batch 105/161, Loss: 0.6182268190951574, Time taken since last batch: 11.63s, Total time taken: 174.21s\n",
      "Epoch 5/20, Batch 112/161, Loss: 0.6182239500007459, Time taken since last batch: 11.71s, Total time taken: 185.92s\n",
      "Epoch 5/20, Batch 119/161, Loss: 0.6128497832462567, Time taken since last batch: 11.69s, Total time taken: 197.62s\n",
      "Epoch 5/20, Batch 126/161, Loss: 0.6079214119485447, Time taken since last batch: 11.48s, Total time taken: 209.10s\n",
      "Epoch 5/20, Batch 133/161, Loss: 0.6093968259436744, Time taken since last batch: 11.45s, Total time taken: 220.55s\n",
      "Epoch 5/20, Batch 140/161, Loss: 0.6098261133900711, Time taken since last batch: 11.73s, Total time taken: 232.28s\n",
      "Epoch 5/20, Batch 147/161, Loss: 0.6069018424165492, Time taken since last batch: 11.61s, Total time taken: 243.89s\n",
      "Epoch 5/20, Batch 154/161, Loss: 0.6022522556897881, Time taken since last batch: 12.08s, Total time taken: 255.97s\n",
      "Epoch 5/20, Batch 161/161, Loss: 0.6021200793315165, Time taken since last batch: 11.60s, Total time taken: 267.57s\n",
      "Epoch 6/20, Batch 7/161, Loss: 0.4710786129747118, Time taken since last batch: 11.66s, Total time taken: 11.66s\n",
      "Epoch 6/20, Batch 14/161, Loss: 0.49747440006051746, Time taken since last batch: 11.57s, Total time taken: 23.23s\n",
      "Epoch 6/20, Batch 21/161, Loss: 0.49104917758987066, Time taken since last batch: 11.78s, Total time taken: 35.01s\n",
      "Epoch 6/20, Batch 28/161, Loss: 0.4862079450062343, Time taken since last batch: 11.49s, Total time taken: 46.49s\n",
      "Epoch 6/20, Batch 35/161, Loss: 0.48198672107287815, Time taken since last batch: 11.61s, Total time taken: 58.10s\n",
      "Epoch 6/20, Batch 42/161, Loss: 0.5023901150340125, Time taken since last batch: 11.75s, Total time taken: 69.86s\n",
      "Epoch 6/20, Batch 49/161, Loss: 0.5267193274838584, Time taken since last batch: 11.56s, Total time taken: 81.42s\n",
      "Epoch 6/20, Batch 56/161, Loss: 0.5253912678786686, Time taken since last batch: 11.76s, Total time taken: 93.18s\n",
      "Epoch 6/20, Batch 63/161, Loss: 0.534849174438961, Time taken since last batch: 11.61s, Total time taken: 104.79s\n",
      "Epoch 6/20, Batch 70/161, Loss: 0.5327501173530306, Time taken since last batch: 11.72s, Total time taken: 116.51s\n",
      "Epoch 6/20, Batch 77/161, Loss: 0.5338541880830542, Time taken since last batch: 11.55s, Total time taken: 128.06s\n",
      "Epoch 6/20, Batch 84/161, Loss: 0.5370150052365803, Time taken since last batch: 11.71s, Total time taken: 139.77s\n",
      "Epoch 6/20, Batch 91/161, Loss: 0.5379999240676125, Time taken since last batch: 11.60s, Total time taken: 151.37s\n",
      "Epoch 6/20, Batch 98/161, Loss: 0.5416109783917057, Time taken since last batch: 11.68s, Total time taken: 163.05s\n",
      "Epoch 6/20, Batch 105/161, Loss: 0.5425086645852952, Time taken since last batch: 11.60s, Total time taken: 174.64s\n",
      "Epoch 6/20, Batch 112/161, Loss: 0.5417113003454038, Time taken since last batch: 11.66s, Total time taken: 186.30s\n",
      "Epoch 6/20, Batch 119/161, Loss: 0.5453882976239469, Time taken since last batch: 11.54s, Total time taken: 197.84s\n",
      "Epoch 6/20, Batch 126/161, Loss: 0.5412642172877751, Time taken since last batch: 11.56s, Total time taken: 209.40s\n",
      "Epoch 6/20, Batch 133/161, Loss: 0.5421706870534366, Time taken since last batch: 11.62s, Total time taken: 221.02s\n",
      "Epoch 6/20, Batch 140/161, Loss: 0.539774960918086, Time taken since last batch: 11.66s, Total time taken: 232.68s\n",
      "Epoch 6/20, Batch 147/161, Loss: 0.5415885811354838, Time taken since last batch: 11.46s, Total time taken: 244.14s\n",
      "Epoch 6/20, Batch 154/161, Loss: 0.5412302290077333, Time taken since last batch: 11.45s, Total time taken: 255.59s\n",
      "Epoch 6/20, Batch 161/161, Loss: 0.5414678598783031, Time taken since last batch: 11.42s, Total time taken: 267.01s\n",
      "Epoch 7/20, Batch 7/161, Loss: 0.5002083906105587, Time taken since last batch: 11.49s, Total time taken: 11.49s\n",
      "Epoch 7/20, Batch 14/161, Loss: 0.5191574799163001, Time taken since last batch: 11.71s, Total time taken: 23.21s\n",
      "Epoch 7/20, Batch 21/161, Loss: 0.5215965821629479, Time taken since last batch: 11.77s, Total time taken: 34.98s\n",
      "Epoch 7/20, Batch 28/161, Loss: 0.4909166784158775, Time taken since last batch: 11.45s, Total time taken: 46.43s\n",
      "Epoch 7/20, Batch 35/161, Loss: 0.49928321284907207, Time taken since last batch: 11.61s, Total time taken: 58.04s\n",
      "Epoch 7/20, Batch 42/161, Loss: 0.5065949576951209, Time taken since last batch: 11.62s, Total time taken: 69.66s\n",
      "Epoch 7/20, Batch 49/161, Loss: 0.502155348050351, Time taken since last batch: 11.94s, Total time taken: 81.60s\n",
      "Epoch 7/20, Batch 56/161, Loss: 0.5063110076423202, Time taken since last batch: 12.53s, Total time taken: 94.12s\n",
      "Epoch 7/20, Batch 63/161, Loss: 0.5065504258114194, Time taken since last batch: 12.23s, Total time taken: 106.36s\n",
      "Epoch 7/20, Batch 70/161, Loss: 0.5176048527870859, Time taken since last batch: 11.89s, Total time taken: 118.25s\n",
      "Epoch 7/20, Batch 77/161, Loss: 0.5167229417469594, Time taken since last batch: 12.00s, Total time taken: 130.24s\n",
      "Epoch 7/20, Batch 84/161, Loss: 0.5238732676066103, Time taken since last batch: 11.87s, Total time taken: 142.11s\n",
      "Epoch 7/20, Batch 91/161, Loss: 0.5265200794725627, Time taken since last batch: 11.87s, Total time taken: 153.98s\n",
      "Epoch 7/20, Batch 98/161, Loss: 0.5248930391912557, Time taken since last batch: 12.04s, Total time taken: 166.02s\n",
      "Epoch 7/20, Batch 105/161, Loss: 0.5202110811358407, Time taken since last batch: 11.81s, Total time taken: 177.83s\n",
      "Epoch 7/20, Batch 112/161, Loss: 0.5189005241596273, Time taken since last batch: 11.92s, Total time taken: 189.76s\n",
      "Epoch 7/20, Batch 119/161, Loss: 0.520146666329448, Time taken since last batch: 11.81s, Total time taken: 201.56s\n",
      "Epoch 7/20, Batch 126/161, Loss: 0.5208282441137329, Time taken since last batch: 12.12s, Total time taken: 213.68s\n",
      "Epoch 7/20, Batch 133/161, Loss: 0.521835831547142, Time taken since last batch: 12.03s, Total time taken: 225.71s\n",
      "Epoch 7/20, Batch 140/161, Loss: 0.5208156330244882, Time taken since last batch: 11.99s, Total time taken: 237.70s\n",
      "Epoch 7/20, Batch 147/161, Loss: 0.5219974463083306, Time taken since last batch: 11.93s, Total time taken: 249.62s\n",
      "Epoch 7/20, Batch 154/161, Loss: 0.5197271296343247, Time taken since last batch: 11.74s, Total time taken: 261.36s\n",
      "Epoch 7/20, Batch 161/161, Loss: 0.5149329360227407, Time taken since last batch: 11.53s, Total time taken: 272.89s\n",
      "Epoch 8/20, Batch 7/161, Loss: 0.3785684108734131, Time taken since last batch: 11.75s, Total time taken: 11.75s\n",
      "Epoch 8/20, Batch 14/161, Loss: 0.40325039838041576, Time taken since last batch: 11.95s, Total time taken: 23.70s\n",
      "Epoch 8/20, Batch 21/161, Loss: 0.3998117446899414, Time taken since last batch: 11.91s, Total time taken: 35.61s\n",
      "Epoch 8/20, Batch 28/161, Loss: 0.42259082943201065, Time taken since last batch: 11.88s, Total time taken: 47.49s\n",
      "Epoch 8/20, Batch 35/161, Loss: 0.4507448724337986, Time taken since last batch: 11.97s, Total time taken: 59.46s\n",
      "Epoch 8/20, Batch 42/161, Loss: 0.4536157654864447, Time taken since last batch: 11.99s, Total time taken: 71.45s\n",
      "Epoch 8/20, Batch 49/161, Loss: 0.45327974521383946, Time taken since last batch: 12.01s, Total time taken: 83.46s\n",
      "Epoch 8/20, Batch 56/161, Loss: 0.45238664586629185, Time taken since last batch: 12.51s, Total time taken: 95.96s\n",
      "Epoch 8/20, Batch 63/161, Loss: 0.45808755547281294, Time taken since last batch: 12.05s, Total time taken: 108.02s\n",
      "Epoch 8/20, Batch 70/161, Loss: 0.4654427170753479, Time taken since last batch: 12.12s, Total time taken: 120.13s\n",
      "Epoch 8/20, Batch 77/161, Loss: 0.46996058968754556, Time taken since last batch: 11.77s, Total time taken: 131.91s\n",
      "Epoch 8/20, Batch 84/161, Loss: 0.47373469989924205, Time taken since last batch: 11.77s, Total time taken: 143.67s\n",
      "Epoch 8/20, Batch 91/161, Loss: 0.48130642156024556, Time taken since last batch: 11.83s, Total time taken: 155.50s\n",
      "Epoch 8/20, Batch 98/161, Loss: 0.4887788490373261, Time taken since last batch: 11.87s, Total time taken: 167.37s\n",
      "Epoch 8/20, Batch 105/161, Loss: 0.48895927724384125, Time taken since last batch: 11.91s, Total time taken: 179.28s\n",
      "Epoch 8/20, Batch 112/161, Loss: 0.4889858096305813, Time taken since last batch: 11.84s, Total time taken: 191.13s\n",
      "Epoch 8/20, Batch 119/161, Loss: 0.4856212313185219, Time taken since last batch: 11.90s, Total time taken: 203.03s\n",
      "Epoch 8/20, Batch 126/161, Loss: 0.482912896999291, Time taken since last batch: 11.92s, Total time taken: 214.95s\n",
      "Epoch 8/20, Batch 133/161, Loss: 0.48547300102567315, Time taken since last batch: 11.80s, Total time taken: 226.76s\n",
      "Epoch 8/20, Batch 140/161, Loss: 0.48239624447056223, Time taken since last batch: 12.01s, Total time taken: 238.76s\n",
      "Epoch 8/20, Batch 147/161, Loss: 0.4862833229862914, Time taken since last batch: 12.06s, Total time taken: 250.82s\n",
      "Epoch 8/20, Batch 154/161, Loss: 0.49024893430533345, Time taken since last batch: 11.80s, Total time taken: 262.62s\n",
      "Epoch 8/20, Batch 161/161, Loss: 0.48893886128937974, Time taken since last batch: 11.55s, Total time taken: 274.17s\n",
      "Epoch 9/20, Batch 7/161, Loss: 0.42698277958801817, Time taken since last batch: 12.10s, Total time taken: 12.10s\n",
      "Epoch 9/20, Batch 14/161, Loss: 0.4218468378697123, Time taken since last batch: 12.00s, Total time taken: 24.10s\n",
      "Epoch 9/20, Batch 21/161, Loss: 0.4188908090194066, Time taken since last batch: 11.89s, Total time taken: 35.99s\n",
      "Epoch 9/20, Batch 28/161, Loss: 0.40644056828958647, Time taken since last batch: 11.98s, Total time taken: 47.98s\n",
      "Epoch 9/20, Batch 35/161, Loss: 0.4211316300289972, Time taken since last batch: 11.70s, Total time taken: 59.68s\n",
      "Epoch 9/20, Batch 42/161, Loss: 0.40857730096294764, Time taken since last batch: 11.96s, Total time taken: 71.65s\n",
      "Epoch 9/20, Batch 49/161, Loss: 0.4119689567964904, Time taken since last batch: 11.81s, Total time taken: 83.45s\n",
      "Epoch 9/20, Batch 56/161, Loss: 0.4151869011776788, Time taken since last batch: 11.82s, Total time taken: 95.27s\n",
      "Epoch 9/20, Batch 63/161, Loss: 0.4154781670797439, Time taken since last batch: 11.98s, Total time taken: 107.26s\n",
      "Epoch 9/20, Batch 70/161, Loss: 0.4304839611053467, Time taken since last batch: 11.70s, Total time taken: 118.95s\n",
      "Epoch 9/20, Batch 77/161, Loss: 0.4294011616087579, Time taken since last batch: 12.06s, Total time taken: 131.01s\n",
      "Epoch 9/20, Batch 84/161, Loss: 0.43883015516968, Time taken since last batch: 11.88s, Total time taken: 142.90s\n",
      "Epoch 9/20, Batch 91/161, Loss: 0.455511253122445, Time taken since last batch: 12.08s, Total time taken: 154.98s\n",
      "Epoch 9/20, Batch 98/161, Loss: 0.4523142936582468, Time taken since last batch: 11.92s, Total time taken: 166.90s\n",
      "Epoch 9/20, Batch 105/161, Loss: 0.4501095214060375, Time taken since last batch: 11.83s, Total time taken: 178.73s\n",
      "Epoch 9/20, Batch 112/161, Loss: 0.4530261358512299, Time taken since last batch: 11.94s, Total time taken: 190.67s\n",
      "Epoch 9/20, Batch 119/161, Loss: 0.44602458467002676, Time taken since last batch: 11.68s, Total time taken: 202.35s\n",
      "Epoch 9/20, Batch 126/161, Loss: 0.4401086559371343, Time taken since last batch: 11.94s, Total time taken: 214.28s\n",
      "Epoch 9/20, Batch 133/161, Loss: 0.44410301957811626, Time taken since last batch: 11.95s, Total time taken: 226.23s\n",
      "Epoch 9/20, Batch 140/161, Loss: 0.4491571620106697, Time taken since last batch: 11.89s, Total time taken: 238.13s\n",
      "Epoch 9/20, Batch 147/161, Loss: 0.44799575422491345, Time taken since last batch: 11.96s, Total time taken: 250.09s\n",
      "Epoch 9/20, Batch 154/161, Loss: 0.4541251933226338, Time taken since last batch: 11.62s, Total time taken: 261.71s\n",
      "Epoch 9/20, Batch 161/161, Loss: 0.45252454975006745, Time taken since last batch: 11.53s, Total time taken: 273.24s\n",
      "Epoch 10/20, Batch 7/161, Loss: 0.4331540082182203, Time taken since last batch: 11.85s, Total time taken: 11.85s\n",
      "Epoch 10/20, Batch 14/161, Loss: 0.4349140354565212, Time taken since last batch: 12.19s, Total time taken: 24.04s\n",
      "Epoch 10/20, Batch 21/161, Loss: 0.40701901132152196, Time taken since last batch: 11.86s, Total time taken: 35.90s\n",
      "Epoch 10/20, Batch 28/161, Loss: 0.3795242714030402, Time taken since last batch: 11.64s, Total time taken: 47.53s\n",
      "Epoch 10/20, Batch 35/161, Loss: 0.3995903151375907, Time taken since last batch: 11.83s, Total time taken: 59.36s\n",
      "Epoch 10/20, Batch 42/161, Loss: 0.4225763983669735, Time taken since last batch: 12.18s, Total time taken: 71.54s\n",
      "Epoch 10/20, Batch 49/161, Loss: 0.4189956306802983, Time taken since last batch: 11.97s, Total time taken: 83.51s\n",
      "Epoch 10/20, Batch 56/161, Loss: 0.4358347106192793, Time taken since last batch: 11.74s, Total time taken: 95.25s\n",
      "Epoch 10/20, Batch 63/161, Loss: 0.43629953891985, Time taken since last batch: 11.68s, Total time taken: 106.93s\n",
      "Epoch 10/20, Batch 70/161, Loss: 0.428021008095571, Time taken since last batch: 11.86s, Total time taken: 118.79s\n",
      "Epoch 10/20, Batch 77/161, Loss: 0.44120684682161776, Time taken since last batch: 11.89s, Total time taken: 130.68s\n",
      "Epoch 10/20, Batch 84/161, Loss: 0.44118705134661423, Time taken since last batch: 12.06s, Total time taken: 142.74s\n",
      "Epoch 10/20, Batch 91/161, Loss: 0.44089659385301255, Time taken since last batch: 11.89s, Total time taken: 154.64s\n",
      "Epoch 10/20, Batch 98/161, Loss: 0.4354495736865365, Time taken since last batch: 11.92s, Total time taken: 166.56s\n",
      "Epoch 10/20, Batch 105/161, Loss: 0.4336935184541203, Time taken since last batch: 11.84s, Total time taken: 178.39s\n",
      "Epoch 10/20, Batch 112/161, Loss: 0.4302619130882834, Time taken since last batch: 11.98s, Total time taken: 190.37s\n",
      "Epoch 10/20, Batch 119/161, Loss: 0.4314708025390361, Time taken since last batch: 11.99s, Total time taken: 202.36s\n",
      "Epoch 10/20, Batch 126/161, Loss: 0.4313996932691052, Time taken since last batch: 11.96s, Total time taken: 214.31s\n",
      "Epoch 10/20, Batch 133/161, Loss: 0.428774004311938, Time taken since last batch: 11.84s, Total time taken: 226.16s\n",
      "Epoch 10/20, Batch 140/161, Loss: 0.4311913464218378, Time taken since last batch: 11.79s, Total time taken: 237.94s\n",
      "Epoch 10/20, Batch 147/161, Loss: 0.4288022811619603, Time taken since last batch: 11.96s, Total time taken: 249.90s\n",
      "Epoch 10/20, Batch 154/161, Loss: 0.42985436721862136, Time taken since last batch: 12.02s, Total time taken: 261.92s\n",
      "Epoch 10/20, Batch 161/161, Loss: 0.4288570133517988, Time taken since last batch: 11.60s, Total time taken: 273.52s\n",
      "Epoch 11/20, Batch 7/161, Loss: 0.3813275822571346, Time taken since last batch: 12.12s, Total time taken: 12.12s\n",
      "Epoch 11/20, Batch 14/161, Loss: 0.39898555619376047, Time taken since last batch: 11.85s, Total time taken: 23.97s\n",
      "Epoch 11/20, Batch 21/161, Loss: 0.39645001008397057, Time taken since last batch: 11.88s, Total time taken: 35.85s\n",
      "Epoch 11/20, Batch 28/161, Loss: 0.4300915226340294, Time taken since last batch: 12.07s, Total time taken: 47.92s\n",
      "Epoch 11/20, Batch 35/161, Loss: 0.4169053997312273, Time taken since last batch: 11.89s, Total time taken: 59.81s\n",
      "Epoch 11/20, Batch 42/161, Loss: 0.4106619648990177, Time taken since last batch: 11.93s, Total time taken: 71.74s\n",
      "Epoch 11/20, Batch 49/161, Loss: 0.40290387278916884, Time taken since last batch: 12.10s, Total time taken: 83.84s\n",
      "Epoch 11/20, Batch 56/161, Loss: 0.4073961464954274, Time taken since last batch: 12.13s, Total time taken: 95.97s\n",
      "Epoch 11/20, Batch 63/161, Loss: 0.4163794534073936, Time taken since last batch: 11.75s, Total time taken: 107.72s\n",
      "Epoch 11/20, Batch 70/161, Loss: 0.411741643505437, Time taken since last batch: 11.94s, Total time taken: 119.66s\n",
      "Epoch 11/20, Batch 77/161, Loss: 0.4070591913028197, Time taken since last batch: 11.94s, Total time taken: 131.60s\n",
      "Epoch 11/20, Batch 84/161, Loss: 0.3948415707619417, Time taken since last batch: 11.90s, Total time taken: 143.50s\n",
      "Epoch 11/20, Batch 91/161, Loss: 0.39617076761774966, Time taken since last batch: 12.03s, Total time taken: 155.52s\n",
      "Epoch 11/20, Batch 98/161, Loss: 0.38862406948999484, Time taken since last batch: 12.08s, Total time taken: 167.61s\n",
      "Epoch 11/20, Batch 105/161, Loss: 0.3886543945187614, Time taken since last batch: 12.36s, Total time taken: 179.97s\n",
      "Epoch 11/20, Batch 112/161, Loss: 0.39074021724185776, Time taken since last batch: 11.84s, Total time taken: 191.81s\n",
      "Epoch 11/20, Batch 119/161, Loss: 0.3915053187799053, Time taken since last batch: 12.03s, Total time taken: 203.85s\n",
      "Epoch 11/20, Batch 126/161, Loss: 0.39672731206057565, Time taken since last batch: 12.00s, Total time taken: 215.84s\n",
      "Epoch 11/20, Batch 133/161, Loss: 0.40081424159663065, Time taken since last batch: 11.93s, Total time taken: 227.77s\n",
      "Epoch 11/20, Batch 140/161, Loss: 0.3988583483866283, Time taken since last batch: 11.85s, Total time taken: 239.62s\n",
      "Epoch 11/20, Batch 147/161, Loss: 0.39883052562775256, Time taken since last batch: 11.91s, Total time taken: 251.53s\n",
      "Epoch 11/20, Batch 154/161, Loss: 0.4003760037677629, Time taken since last batch: 11.95s, Total time taken: 263.48s\n",
      "Epoch 11/20, Batch 161/161, Loss: 0.4017833852619858, Time taken since last batch: 11.70s, Total time taken: 275.19s\n",
      "Epoch 12/20, Batch 7/161, Loss: 0.4189567140170506, Time taken since last batch: 12.02s, Total time taken: 12.02s\n",
      "Epoch 12/20, Batch 14/161, Loss: 0.45459129767758505, Time taken since last batch: 11.98s, Total time taken: 24.00s\n",
      "Epoch 12/20, Batch 21/161, Loss: 0.4747557725225176, Time taken since last batch: 11.86s, Total time taken: 35.85s\n",
      "Epoch 12/20, Batch 28/161, Loss: 0.4805480795247214, Time taken since last batch: 11.91s, Total time taken: 47.76s\n",
      "Epoch 12/20, Batch 35/161, Loss: 0.4542409139020102, Time taken since last batch: 12.06s, Total time taken: 59.83s\n",
      "Epoch 12/20, Batch 42/161, Loss: 0.43669505204473225, Time taken since last batch: 11.81s, Total time taken: 71.64s\n",
      "Epoch 12/20, Batch 49/161, Loss: 0.42732677897628474, Time taken since last batch: 12.15s, Total time taken: 83.79s\n",
      "Epoch 12/20, Batch 56/161, Loss: 0.42511616833508015, Time taken since last batch: 11.94s, Total time taken: 95.73s\n",
      "Epoch 12/20, Batch 63/161, Loss: 0.4198982048602331, Time taken since last batch: 11.99s, Total time taken: 107.73s\n",
      "Epoch 12/20, Batch 70/161, Loss: 0.4086792635066169, Time taken since last batch: 11.98s, Total time taken: 119.70s\n",
      "Epoch 12/20, Batch 77/161, Loss: 0.3998814830919365, Time taken since last batch: 11.88s, Total time taken: 131.58s\n",
      "Epoch 12/20, Batch 84/161, Loss: 0.39422251905004185, Time taken since last batch: 12.08s, Total time taken: 143.66s\n",
      "Epoch 12/20, Batch 91/161, Loss: 0.40205231489061, Time taken since last batch: 11.83s, Total time taken: 155.48s\n",
      "Epoch 12/20, Batch 98/161, Loss: 0.3950209941486923, Time taken since last batch: 11.80s, Total time taken: 167.28s\n",
      "Epoch 12/20, Batch 105/161, Loss: 0.399577281446684, Time taken since last batch: 12.17s, Total time taken: 179.46s\n",
      "Epoch 12/20, Batch 112/161, Loss: 0.40501811714576824, Time taken since last batch: 12.22s, Total time taken: 191.67s\n",
      "Epoch 12/20, Batch 119/161, Loss: 0.4032955696853269, Time taken since last batch: 11.88s, Total time taken: 203.56s\n",
      "Epoch 12/20, Batch 126/161, Loss: 0.40341721782608636, Time taken since last batch: 11.90s, Total time taken: 215.46s\n",
      "Epoch 12/20, Batch 133/161, Loss: 0.4002767207479118, Time taken since last batch: 11.84s, Total time taken: 227.29s\n",
      "Epoch 12/20, Batch 140/161, Loss: 0.40438762136868067, Time taken since last batch: 12.04s, Total time taken: 239.33s\n",
      "Epoch 12/20, Batch 147/161, Loss: 0.4024200857091112, Time taken since last batch: 11.80s, Total time taken: 251.13s\n",
      "Epoch 12/20, Batch 154/161, Loss: 0.4031905136131621, Time taken since last batch: 11.94s, Total time taken: 263.07s\n",
      "Epoch 12/20, Batch 161/161, Loss: 0.4008865114879904, Time taken since last batch: 11.61s, Total time taken: 274.69s\n",
      "Epoch 13/20, Batch 7/161, Loss: 0.41740986491952625, Time taken since last batch: 11.93s, Total time taken: 11.93s\n",
      "Epoch 13/20, Batch 14/161, Loss: 0.36545243646417347, Time taken since last batch: 12.11s, Total time taken: 24.04s\n",
      "Epoch 13/20, Batch 21/161, Loss: 0.3736282821212496, Time taken since last batch: 11.89s, Total time taken: 35.93s\n",
      "Epoch 13/20, Batch 28/161, Loss: 0.3704926052263805, Time taken since last batch: 11.88s, Total time taken: 47.81s\n",
      "Epoch 13/20, Batch 35/161, Loss: 0.3680367661373956, Time taken since last batch: 11.94s, Total time taken: 59.76s\n",
      "Epoch 13/20, Batch 42/161, Loss: 0.3712927397517931, Time taken since last batch: 11.94s, Total time taken: 71.70s\n",
      "Epoch 13/20, Batch 49/161, Loss: 0.3754756277313038, Time taken since last batch: 11.96s, Total time taken: 83.66s\n",
      "Epoch 13/20, Batch 56/161, Loss: 0.37262933275529314, Time taken since last batch: 11.90s, Total time taken: 95.56s\n",
      "Epoch 13/20, Batch 63/161, Loss: 0.38195937540796066, Time taken since last batch: 11.86s, Total time taken: 107.42s\n",
      "Epoch 13/20, Batch 70/161, Loss: 0.37584552126271387, Time taken since last batch: 12.01s, Total time taken: 119.43s\n",
      "Epoch 13/20, Batch 77/161, Loss: 0.371315016181438, Time taken since last batch: 12.04s, Total time taken: 131.47s\n",
      "Epoch 13/20, Batch 84/161, Loss: 0.36925648365701946, Time taken since last batch: 12.05s, Total time taken: 143.52s\n",
      "Epoch 13/20, Batch 91/161, Loss: 0.3733008950948715, Time taken since last batch: 11.93s, Total time taken: 155.45s\n",
      "Epoch 13/20, Batch 98/161, Loss: 0.3738868450936006, Time taken since last batch: 11.90s, Total time taken: 167.34s\n",
      "Epoch 13/20, Batch 105/161, Loss: 0.3750563810269038, Time taken since last batch: 11.79s, Total time taken: 179.13s\n",
      "Epoch 13/20, Batch 112/161, Loss: 0.3758125538006425, Time taken since last batch: 12.03s, Total time taken: 191.17s\n",
      "Epoch 13/20, Batch 119/161, Loss: 0.3771438122797413, Time taken since last batch: 11.96s, Total time taken: 203.13s\n",
      "Epoch 13/20, Batch 126/161, Loss: 0.3771838163809171, Time taken since last batch: 11.81s, Total time taken: 214.93s\n",
      "Epoch 13/20, Batch 133/161, Loss: 0.3753312725321691, Time taken since last batch: 11.94s, Total time taken: 226.87s\n",
      "Epoch 13/20, Batch 140/161, Loss: 0.37052107623645236, Time taken since last batch: 11.95s, Total time taken: 238.82s\n",
      "Epoch 13/20, Batch 147/161, Loss: 0.3688402951371913, Time taken since last batch: 11.70s, Total time taken: 250.52s\n",
      "Epoch 13/20, Batch 154/161, Loss: 0.3679302948248851, Time taken since last batch: 12.09s, Total time taken: 262.62s\n",
      "Epoch 13/20, Batch 161/161, Loss: 0.3717914579077537, Time taken since last batch: 11.91s, Total time taken: 274.53s\n",
      "Epoch 14/20, Batch 7/161, Loss: 0.32292090356349945, Time taken since last batch: 11.73s, Total time taken: 11.73s\n",
      "Epoch 14/20, Batch 14/161, Loss: 0.3369665795138904, Time taken since last batch: 11.88s, Total time taken: 23.62s\n",
      "Epoch 14/20, Batch 21/161, Loss: 0.33528504201344084, Time taken since last batch: 11.90s, Total time taken: 35.52s\n",
      "Epoch 14/20, Batch 28/161, Loss: 0.34565876690404757, Time taken since last batch: 11.79s, Total time taken: 47.31s\n",
      "Epoch 14/20, Batch 35/161, Loss: 0.35711072172437397, Time taken since last batch: 12.12s, Total time taken: 59.43s\n",
      "Epoch 14/20, Batch 42/161, Loss: 0.35478151660589946, Time taken since last batch: 11.98s, Total time taken: 71.41s\n",
      "Epoch 14/20, Batch 49/161, Loss: 0.34737294030432797, Time taken since last batch: 11.80s, Total time taken: 83.21s\n",
      "Epoch 14/20, Batch 56/161, Loss: 0.34617170558444094, Time taken since last batch: 12.02s, Total time taken: 95.22s\n",
      "Epoch 14/20, Batch 63/161, Loss: 0.33842095732688904, Time taken since last batch: 11.98s, Total time taken: 107.20s\n",
      "Epoch 14/20, Batch 70/161, Loss: 0.332087855892522, Time taken since last batch: 11.94s, Total time taken: 119.15s\n",
      "Epoch 14/20, Batch 77/161, Loss: 0.34714319934318594, Time taken since last batch: 12.16s, Total time taken: 131.30s\n",
      "Epoch 14/20, Batch 84/161, Loss: 0.3525780134257816, Time taken since last batch: 11.72s, Total time taken: 143.02s\n",
      "Epoch 14/20, Batch 91/161, Loss: 0.3499922044984587, Time taken since last batch: 11.97s, Total time taken: 154.99s\n",
      "Epoch 14/20, Batch 98/161, Loss: 0.35509596917094016, Time taken since last batch: 12.08s, Total time taken: 167.07s\n",
      "Epoch 14/20, Batch 105/161, Loss: 0.35298452973365785, Time taken since last batch: 12.06s, Total time taken: 179.13s\n",
      "Epoch 14/20, Batch 112/161, Loss: 0.35672464847032515, Time taken since last batch: 11.76s, Total time taken: 190.90s\n",
      "Epoch 14/20, Batch 119/161, Loss: 0.3575355403062676, Time taken since last batch: 11.95s, Total time taken: 202.85s\n",
      "Epoch 14/20, Batch 126/161, Loss: 0.35534561295357964, Time taken since last batch: 12.17s, Total time taken: 215.01s\n",
      "Epoch 14/20, Batch 133/161, Loss: 0.3537462714471315, Time taken since last batch: 11.82s, Total time taken: 226.84s\n",
      "Epoch 14/20, Batch 140/161, Loss: 0.352441992610693, Time taken since last batch: 12.05s, Total time taken: 238.89s\n",
      "Epoch 14/20, Batch 147/161, Loss: 0.35097689896213763, Time taken since last batch: 12.29s, Total time taken: 251.17s\n",
      "Epoch 14/20, Batch 154/161, Loss: 0.34665371322786653, Time taken since last batch: 12.15s, Total time taken: 263.32s\n",
      "Epoch 14/20, Batch 161/161, Loss: 0.34995265841854284, Time taken since last batch: 11.71s, Total time taken: 275.03s\n",
      "Epoch 15/20, Batch 7/161, Loss: 0.2951941341161728, Time taken since last batch: 11.94s, Total time taken: 11.94s\n",
      "Epoch 15/20, Batch 14/161, Loss: 0.33419850575072424, Time taken since last batch: 11.98s, Total time taken: 23.92s\n",
      "Epoch 15/20, Batch 21/161, Loss: 0.33272628557114375, Time taken since last batch: 12.26s, Total time taken: 36.18s\n",
      "Epoch 15/20, Batch 28/161, Loss: 0.3237134244825159, Time taken since last batch: 12.15s, Total time taken: 48.33s\n",
      "Epoch 15/20, Batch 35/161, Loss: 0.3343201083796365, Time taken since last batch: 11.86s, Total time taken: 60.19s\n",
      "Epoch 15/20, Batch 42/161, Loss: 0.3261581371937479, Time taken since last batch: 11.96s, Total time taken: 72.15s\n",
      "Epoch 15/20, Batch 49/161, Loss: 0.3333315517829389, Time taken since last batch: 12.01s, Total time taken: 84.16s\n",
      "Epoch 15/20, Batch 56/161, Loss: 0.33394160494208336, Time taken since last batch: 11.84s, Total time taken: 96.01s\n",
      "Epoch 15/20, Batch 63/161, Loss: 0.33251706948356025, Time taken since last batch: 11.85s, Total time taken: 107.85s\n",
      "Epoch 15/20, Batch 70/161, Loss: 0.3249735554414136, Time taken since last batch: 12.29s, Total time taken: 120.14s\n",
      "Epoch 15/20, Batch 77/161, Loss: 0.32670781722703535, Time taken since last batch: 11.99s, Total time taken: 132.13s\n",
      "Epoch 15/20, Batch 84/161, Loss: 0.33209104951293694, Time taken since last batch: 11.85s, Total time taken: 143.99s\n",
      "Epoch 15/20, Batch 91/161, Loss: 0.33001617276734047, Time taken since last batch: 11.85s, Total time taken: 155.84s\n",
      "Epoch 15/20, Batch 98/161, Loss: 0.3318917169710811, Time taken since last batch: 11.99s, Total time taken: 167.83s\n",
      "Epoch 15/20, Batch 105/161, Loss: 0.3376543023046993, Time taken since last batch: 11.99s, Total time taken: 179.81s\n",
      "Epoch 15/20, Batch 112/161, Loss: 0.33402504419375745, Time taken since last batch: 11.99s, Total time taken: 191.81s\n",
      "Epoch 15/20, Batch 119/161, Loss: 0.3345843417679562, Time taken since last batch: 12.00s, Total time taken: 203.80s\n",
      "Epoch 15/20, Batch 126/161, Loss: 0.3377654765924764, Time taken since last batch: 11.89s, Total time taken: 215.70s\n",
      "Epoch 15/20, Batch 133/161, Loss: 0.3332344777041808, Time taken since last batch: 11.74s, Total time taken: 227.44s\n",
      "Epoch 15/20, Batch 140/161, Loss: 0.3385841511722122, Time taken since last batch: 12.08s, Total time taken: 239.51s\n",
      "Epoch 15/20, Batch 147/161, Loss: 0.3407773633493858, Time taken since last batch: 12.14s, Total time taken: 251.65s\n",
      "Epoch 15/20, Batch 154/161, Loss: 0.3415161212059585, Time taken since last batch: 12.16s, Total time taken: 263.81s\n",
      "Epoch 15/20, Batch 161/161, Loss: 0.34229494099106106, Time taken since last batch: 11.82s, Total time taken: 275.63s\n",
      "Epoch 16/20, Batch 7/161, Loss: 0.27855918237141203, Time taken since last batch: 11.90s, Total time taken: 11.90s\n",
      "Epoch 16/20, Batch 14/161, Loss: 0.27594924505267826, Time taken since last batch: 12.10s, Total time taken: 24.00s\n",
      "Epoch 16/20, Batch 21/161, Loss: 0.2994442078329268, Time taken since last batch: 12.24s, Total time taken: 36.24s\n",
      "Epoch 16/20, Batch 28/161, Loss: 0.3012600152620247, Time taken since last batch: 12.07s, Total time taken: 48.31s\n",
      "Epoch 16/20, Batch 35/161, Loss: 0.3023888860430036, Time taken since last batch: 11.98s, Total time taken: 60.29s\n",
      "Epoch 16/20, Batch 42/161, Loss: 0.30879774867069154, Time taken since last batch: 11.77s, Total time taken: 72.05s\n",
      "Epoch 16/20, Batch 49/161, Loss: 0.31077045278281584, Time taken since last batch: 11.99s, Total time taken: 84.04s\n",
      "Epoch 16/20, Batch 56/161, Loss: 0.3335704557331545, Time taken since last batch: 12.01s, Total time taken: 96.06s\n",
      "Epoch 16/20, Batch 63/161, Loss: 0.3337649990405355, Time taken since last batch: 12.04s, Total time taken: 108.10s\n",
      "Epoch 16/20, Batch 70/161, Loss: 0.33541706513081276, Time taken since last batch: 11.98s, Total time taken: 120.08s\n",
      "Epoch 16/20, Batch 77/161, Loss: 0.33251596938867073, Time taken since last batch: 12.01s, Total time taken: 132.09s\n",
      "Epoch 16/20, Batch 84/161, Loss: 0.3309984365921645, Time taken since last batch: 12.05s, Total time taken: 144.14s\n",
      "Epoch 16/20, Batch 91/161, Loss: 0.33359378224218283, Time taken since last batch: 12.16s, Total time taken: 156.30s\n",
      "Epoch 16/20, Batch 98/161, Loss: 0.34556974569449617, Time taken since last batch: 12.04s, Total time taken: 168.34s\n",
      "Epoch 16/20, Batch 105/161, Loss: 0.3445985002886681, Time taken since last batch: 12.13s, Total time taken: 180.47s\n",
      "Epoch 16/20, Batch 112/161, Loss: 0.3480496266191559, Time taken since last batch: 11.92s, Total time taken: 192.39s\n",
      "Epoch 16/20, Batch 119/161, Loss: 0.3443448433981222, Time taken since last batch: 11.85s, Total time taken: 204.25s\n",
      "Epoch 16/20, Batch 126/161, Loss: 0.34496287084997646, Time taken since last batch: 11.85s, Total time taken: 216.10s\n",
      "Epoch 16/20, Batch 133/161, Loss: 0.3417669562132735, Time taken since last batch: 11.90s, Total time taken: 227.99s\n",
      "Epoch 16/20, Batch 140/161, Loss: 0.3394151339041335, Time taken since last batch: 12.04s, Total time taken: 240.03s\n",
      "Epoch 16/20, Batch 147/161, Loss: 0.3399952689097041, Time taken since last batch: 11.91s, Total time taken: 251.94s\n",
      "Epoch 16/20, Batch 154/161, Loss: 0.34391502092604515, Time taken since last batch: 11.99s, Total time taken: 263.93s\n",
      "Epoch 16/20, Batch 161/161, Loss: 0.34413724499089376, Time taken since last batch: 11.62s, Total time taken: 275.55s\n",
      "Epoch 17/20, Batch 7/161, Loss: 0.32389738730021883, Time taken since last batch: 12.07s, Total time taken: 12.07s\n",
      "Epoch 17/20, Batch 14/161, Loss: 0.34984825764383587, Time taken since last batch: 11.94s, Total time taken: 24.01s\n",
      "Epoch 17/20, Batch 21/161, Loss: 0.35694858573731925, Time taken since last batch: 12.12s, Total time taken: 36.12s\n",
      "Epoch 17/20, Batch 28/161, Loss: 0.3402203462485756, Time taken since last batch: 11.99s, Total time taken: 48.11s\n",
      "Epoch 17/20, Batch 35/161, Loss: 0.3469112164207867, Time taken since last batch: 12.06s, Total time taken: 60.17s\n",
      "Epoch 17/20, Batch 42/161, Loss: 0.32993871346116066, Time taken since last batch: 12.17s, Total time taken: 72.34s\n",
      "Epoch 17/20, Batch 49/161, Loss: 0.32913372635233157, Time taken since last batch: 12.13s, Total time taken: 84.47s\n",
      "Epoch 17/20, Batch 56/161, Loss: 0.32673970463552643, Time taken since last batch: 11.98s, Total time taken: 96.45s\n",
      "Epoch 17/20, Batch 63/161, Loss: 0.33005302399396896, Time taken since last batch: 11.98s, Total time taken: 108.43s\n",
      "Epoch 17/20, Batch 70/161, Loss: 0.32931651824287006, Time taken since last batch: 11.98s, Total time taken: 120.41s\n",
      "Epoch 17/20, Batch 77/161, Loss: 0.32279568010723436, Time taken since last batch: 11.84s, Total time taken: 132.25s\n",
      "Epoch 17/20, Batch 84/161, Loss: 0.3208609220704862, Time taken since last batch: 12.05s, Total time taken: 144.29s\n",
      "Epoch 17/20, Batch 91/161, Loss: 0.31563194137025663, Time taken since last batch: 11.89s, Total time taken: 156.18s\n",
      "Epoch 17/20, Batch 98/161, Loss: 0.31442211370687095, Time taken since last batch: 12.05s, Total time taken: 168.23s\n",
      "Epoch 17/20, Batch 105/161, Loss: 0.3143551731393451, Time taken since last batch: 11.93s, Total time taken: 180.16s\n",
      "Epoch 17/20, Batch 112/161, Loss: 0.31525555199810434, Time taken since last batch: 12.03s, Total time taken: 192.19s\n",
      "Epoch 17/20, Batch 119/161, Loss: 0.3153215669283346, Time taken since last batch: 11.89s, Total time taken: 204.08s\n",
      "Epoch 17/20, Batch 126/161, Loss: 0.31015190080044763, Time taken since last batch: 11.83s, Total time taken: 215.91s\n",
      "Epoch 17/20, Batch 133/161, Loss: 0.3125435087017547, Time taken since last batch: 11.86s, Total time taken: 227.78s\n",
      "Epoch 17/20, Batch 140/161, Loss: 0.31559215379612787, Time taken since last batch: 12.04s, Total time taken: 239.82s\n",
      "Epoch 17/20, Batch 147/161, Loss: 0.3116755541287312, Time taken since last batch: 11.95s, Total time taken: 251.78s\n",
      "Epoch 17/20, Batch 154/161, Loss: 0.30858769393586494, Time taken since last batch: 14.97s, Total time taken: 266.75s\n",
      "Epoch 17/20, Batch 161/161, Loss: 0.3096333409873595, Time taken since last batch: 11.73s, Total time taken: 278.48s\n",
      "Epoch 18/20, Batch 7/161, Loss: 0.2873989375574248, Time taken since last batch: 11.96s, Total time taken: 11.96s\n",
      "Epoch 18/20, Batch 14/161, Loss: 0.32982003848467556, Time taken since last batch: 11.90s, Total time taken: 23.86s\n",
      "Epoch 18/20, Batch 21/161, Loss: 0.2974045280189741, Time taken since last batch: 11.99s, Total time taken: 35.85s\n",
      "Epoch 18/20, Batch 28/161, Loss: 0.2711863076048238, Time taken since last batch: 12.00s, Total time taken: 47.85s\n",
      "Epoch 18/20, Batch 35/161, Loss: 0.29431758778435846, Time taken since last batch: 12.54s, Total time taken: 60.40s\n",
      "Epoch 18/20, Batch 42/161, Loss: 0.28613422704594477, Time taken since last batch: 12.17s, Total time taken: 72.57s\n",
      "Epoch 18/20, Batch 49/161, Loss: 0.2775632489700707, Time taken since last batch: 12.10s, Total time taken: 84.67s\n",
      "Epoch 18/20, Batch 56/161, Loss: 0.280386115424335, Time taken since last batch: 12.00s, Total time taken: 96.67s\n",
      "Epoch 18/20, Batch 63/161, Loss: 0.27922438972053076, Time taken since last batch: 12.09s, Total time taken: 108.76s\n",
      "Epoch 18/20, Batch 70/161, Loss: 0.2830240180449826, Time taken since last batch: 12.08s, Total time taken: 120.84s\n",
      "Epoch 18/20, Batch 77/161, Loss: 0.283972966303299, Time taken since last batch: 11.88s, Total time taken: 132.71s\n",
      "Epoch 18/20, Batch 84/161, Loss: 0.28527242060573327, Time taken since last batch: 11.94s, Total time taken: 144.66s\n",
      "Epoch 18/20, Batch 91/161, Loss: 0.28729285160591317, Time taken since last batch: 11.84s, Total time taken: 156.49s\n",
      "Epoch 18/20, Batch 98/161, Loss: 0.2903322832164716, Time taken since last batch: 12.01s, Total time taken: 168.51s\n",
      "Epoch 18/20, Batch 105/161, Loss: 0.2899146028217815, Time taken since last batch: 12.17s, Total time taken: 180.68s\n",
      "Epoch 18/20, Batch 112/161, Loss: 0.2909734204544553, Time taken since last batch: 12.14s, Total time taken: 192.82s\n",
      "Epoch 18/20, Batch 119/161, Loss: 0.2923914926017032, Time taken since last batch: 11.96s, Total time taken: 204.78s\n",
      "Epoch 18/20, Batch 126/161, Loss: 0.29075361794186017, Time taken since last batch: 12.10s, Total time taken: 216.88s\n",
      "Epoch 18/20, Batch 133/161, Loss: 0.2954271137714386, Time taken since last batch: 11.94s, Total time taken: 228.82s\n",
      "Epoch 18/20, Batch 140/161, Loss: 0.29720659075038774, Time taken since last batch: 12.00s, Total time taken: 240.82s\n",
      "Epoch 18/20, Batch 147/161, Loss: 0.2970463282802478, Time taken since last batch: 11.96s, Total time taken: 252.78s\n",
      "Epoch 18/20, Batch 154/161, Loss: 0.2956969045199357, Time taken since last batch: 12.17s, Total time taken: 264.95s\n",
      "Epoch 18/20, Batch 161/161, Loss: 0.2950574614524101, Time taken since last batch: 11.91s, Total time taken: 276.86s\n",
      "Epoch 19/20, Batch 7/161, Loss: 0.23381919945989335, Time taken since last batch: 11.98s, Total time taken: 11.98s\n",
      "Epoch 19/20, Batch 14/161, Loss: 0.25135882411684307, Time taken since last batch: 11.98s, Total time taken: 23.96s\n",
      "Epoch 19/20, Batch 21/161, Loss: 0.2911172750450316, Time taken since last batch: 12.08s, Total time taken: 36.04s\n",
      "Epoch 19/20, Batch 28/161, Loss: 0.2842095377189772, Time taken since last batch: 11.89s, Total time taken: 47.93s\n",
      "Epoch 19/20, Batch 35/161, Loss: 0.2925247298819678, Time taken since last batch: 11.86s, Total time taken: 59.79s\n",
      "Epoch 19/20, Batch 42/161, Loss: 0.2890736150244872, Time taken since last batch: 12.04s, Total time taken: 71.83s\n",
      "Epoch 19/20, Batch 49/161, Loss: 0.29039234485553234, Time taken since last batch: 12.07s, Total time taken: 83.90s\n",
      "Epoch 19/20, Batch 56/161, Loss: 0.30357231638793436, Time taken since last batch: 11.97s, Total time taken: 95.88s\n",
      "Epoch 19/20, Batch 63/161, Loss: 0.3180070042846695, Time taken since last batch: 11.82s, Total time taken: 107.70s\n",
      "Epoch 19/20, Batch 70/161, Loss: 0.32659306834850993, Time taken since last batch: 11.67s, Total time taken: 119.37s\n",
      "Epoch 19/20, Batch 77/161, Loss: 0.3227307843891057, Time taken since last batch: 12.03s, Total time taken: 131.41s\n",
      "Epoch 19/20, Batch 84/161, Loss: 0.32332623608055566, Time taken since last batch: 11.98s, Total time taken: 143.39s\n",
      "Epoch 19/20, Batch 91/161, Loss: 0.3228902823322422, Time taken since last batch: 11.97s, Total time taken: 155.36s\n",
      "Epoch 19/20, Batch 98/161, Loss: 0.31645680796735143, Time taken since last batch: 12.14s, Total time taken: 167.50s\n",
      "Epoch 19/20, Batch 105/161, Loss: 0.31439896346557705, Time taken since last batch: 12.06s, Total time taken: 179.56s\n",
      "Epoch 19/20, Batch 112/161, Loss: 0.30766011302226354, Time taken since last batch: 12.04s, Total time taken: 191.60s\n",
      "Epoch 19/20, Batch 119/161, Loss: 0.3059617325413127, Time taken since last batch: 12.10s, Total time taken: 203.70s\n",
      "Epoch 19/20, Batch 126/161, Loss: 0.29968202031320995, Time taken since last batch: 11.96s, Total time taken: 215.65s\n",
      "Epoch 19/20, Batch 133/161, Loss: 0.2990745518888746, Time taken since last batch: 11.86s, Total time taken: 227.51s\n",
      "Epoch 19/20, Batch 140/161, Loss: 0.29808278626629287, Time taken since last batch: 11.89s, Total time taken: 239.40s\n",
      "Epoch 19/20, Batch 147/161, Loss: 0.295022507103122, Time taken since last batch: 11.85s, Total time taken: 251.25s\n",
      "Epoch 19/20, Batch 154/161, Loss: 0.29143849005559824, Time taken since last batch: 12.02s, Total time taken: 263.26s\n",
      "Epoch 19/20, Batch 161/161, Loss: 0.29714952529587363, Time taken since last batch: 11.48s, Total time taken: 274.74s\n",
      "Epoch 20/20, Batch 7/161, Loss: 0.244005201118333, Time taken since last batch: 12.03s, Total time taken: 12.03s\n",
      "Epoch 20/20, Batch 14/161, Loss: 0.24665903193610056, Time taken since last batch: 12.19s, Total time taken: 24.22s\n",
      "Epoch 20/20, Batch 21/161, Loss: 0.25662676777158466, Time taken since last batch: 11.96s, Total time taken: 36.18s\n",
      "Epoch 20/20, Batch 28/161, Loss: 0.27104530536702703, Time taken since last batch: 11.93s, Total time taken: 48.11s\n",
      "Epoch 20/20, Batch 35/161, Loss: 0.2747317799500057, Time taken since last batch: 11.92s, Total time taken: 60.04s\n",
      "Epoch 20/20, Batch 42/161, Loss: 0.2793677066053663, Time taken since last batch: 11.97s, Total time taken: 72.00s\n",
      "Epoch 20/20, Batch 49/161, Loss: 0.2782392614350027, Time taken since last batch: 12.00s, Total time taken: 84.00s\n",
      "Epoch 20/20, Batch 56/161, Loss: 0.27475481746452196, Time taken since last batch: 12.18s, Total time taken: 96.18s\n",
      "Epoch 20/20, Batch 63/161, Loss: 0.27050897620973136, Time taken since last batch: 11.96s, Total time taken: 108.14s\n",
      "Epoch 20/20, Batch 70/161, Loss: 0.28316729749952047, Time taken since last batch: 11.95s, Total time taken: 120.10s\n",
      "Epoch 20/20, Batch 77/161, Loss: 0.2841524559182006, Time taken since last batch: 12.19s, Total time taken: 132.29s\n",
      "Epoch 20/20, Batch 84/161, Loss: 0.28421697818807196, Time taken since last batch: 12.25s, Total time taken: 144.53s\n",
      "Epoch 20/20, Batch 91/161, Loss: 0.28341825281853206, Time taken since last batch: 11.84s, Total time taken: 156.38s\n",
      "Epoch 20/20, Batch 98/161, Loss: 0.2821954471268216, Time taken since last batch: 11.98s, Total time taken: 168.36s\n",
      "Epoch 20/20, Batch 105/161, Loss: 0.2848655630435262, Time taken since last batch: 11.75s, Total time taken: 180.10s\n",
      "Epoch 20/20, Batch 112/161, Loss: 0.28366523255993215, Time taken since last batch: 12.15s, Total time taken: 192.25s\n",
      "Epoch 20/20, Batch 119/161, Loss: 0.28863586266251173, Time taken since last batch: 11.98s, Total time taken: 204.24s\n",
      "Epoch 20/20, Batch 126/161, Loss: 0.28701333837613224, Time taken since last batch: 11.94s, Total time taken: 216.18s\n",
      "Epoch 20/20, Batch 133/161, Loss: 0.2836929772581373, Time taken since last batch: 11.89s, Total time taken: 228.08s\n",
      "Epoch 20/20, Batch 140/161, Loss: 0.2800673484270062, Time taken since last batch: 11.94s, Total time taken: 240.02s\n",
      "Epoch 20/20, Batch 147/161, Loss: 0.2785073898884715, Time taken since last batch: 12.14s, Total time taken: 252.16s\n",
      "Epoch 20/20, Batch 154/161, Loss: 0.27713958086905544, Time taken since last batch: 11.98s, Total time taken: 264.14s\n",
      "Epoch 20/20, Batch 161/161, Loss: 0.27534810051044323, Time taken since last batch: 11.76s, Total time taken: 275.90s\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "else:\n",
    "    print(\"GPU is not available. Switching to CPU.\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Function to train the model\n",
    "def train(model, train_loader, optimizer, criterion, num_epochs=5, print_every=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        initial_time = time()\n",
    "        prev_time = initial_time\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Move inputs and labels to GPU\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs, *_ = model(inputs)\n",
    "            loss = criterion(outputs, labels.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Print the loss every print_every batches\n",
    "            if i % print_every == 0 or i == len(train_loader):\n",
    "                avg_loss = running_loss / i\n",
    "                print(f'Epoch {epoch + 1}/{num_epochs}, Batch {i}/{len(train_loader)}, Loss: {avg_loss}, Time taken since last batch: {time() - prev_time:.2f}s, Total time taken: {time() - initial_time:.2f}s')\n",
    "                prev_time = time()\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 20  # You can adjust this value\n",
    "print_every = 7  # Print loss every 10 batches\n",
    "train(model, train_loader, optimizer, criterion, num_epochs, print_every)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T07:04:08.733447500Z",
     "start_time": "2024-01-31T05:31:47.484171800Z"
    }
   },
   "id": "ce250269c784056d",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.21%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "predicted_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Move inputs and labels to GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        labels = labels.flatten()\n",
    "        predicted = predicted.flatten()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Append the predicted labels to the list\n",
    "        predicted_labels.extend(predicted.tolist())\n",
    "\n",
    "# Convert predicted labels to their corresponding class names\n",
    "class_mapping = {0: 'normal', 1: 'fire', 2: 'flood', 3: 'collapsed_buildings', 4: 'car_accident'}\n",
    "predicted_labels = [class_mapping[i] for i in predicted_labels]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {100 * accuracy:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-31T12:20:32.369784500Z",
     "start_time": "2024-01-31T12:19:54.803022500Z"
    }
   },
   "id": "50e9027f1d589da0",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8c50a4ecaebb31b1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
